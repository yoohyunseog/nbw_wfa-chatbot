# -*- coding: utf-8 -*-
"""
ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
ë©”ì¸ íŒŒì¼ì—ì„œ ë¶„ë¦¬ëœ í•¨ìˆ˜ë“¤ì„ ëª¨ì•„ë‘” ëª¨ë“ˆ
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime
import os
import re
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

# moviepyëŠ” ì¡°ê±´ë¶€ë¡œ import
try:
    from moviepy.editor import VideoFileClip
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    print("âš ï¸ moviepy ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ ë³€í™˜ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# PlaywrightëŠ” ì¡°ê±´ë¶€ë¡œ import
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ playwright ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ì›¹ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")


def collect_google_trends():
    """
    êµ¬ê¸€ íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë‰´ìŠ¤ ì œëª©ë§Œ ë°˜í™˜ (ìµœì í™” ë²„ì „)
    
    Returns:
        str: ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‰´ìŠ¤ ì œëª© ë¬¸ìì—´
    """
    try:
        url = "https://trends.google.co.kr/trending/rss?geo=KR"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive'
        }
        
        print("ğŸ” êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì—°ê²° í’€ ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        session = requests.Session()
        session.headers.update(headers)
        
        # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶• (5ì´ˆ)
        response = session.get(url, timeout=5)
        response.raise_for_status()
        
        # ì‘ë‹µ í¬ê¸° ì²´í¬ (ë„ˆë¬´ í° ì‘ë‹µ ë°©ì§€)
        if len(response.content) > 1024 * 1024:  # 1MB ì œí•œ
            print("âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return ""
        
        # XML íŒŒì‹± (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬)
        root = ET.fromstring(response.content)
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
        namespaces = {
            'ht': 'https://trends.google.com/trending/rss'
        }
        
        # RSS ì•„ì´í…œì—ì„œ ë‰´ìŠ¤ ì œëª©ë§Œ ì¶”ì¶œ (ìµœëŒ€ 5ê°œë¡œ ì œí•œ)
        news_titles = []
        items = root.findall('.//item')[:5]  # ìµœëŒ€ 5ê°œë§Œ ì²˜ë¦¬
        
        for item in items:
            # ì²« ë²ˆì§¸ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©)
            news_title_elem = item.find('.//ht:news_item_title', namespaces)
            if news_title_elem is not None and news_title_elem.text:
                news_title = news_title_elem.text.strip()
                if news_title and len(news_title) > 1 and len(news_title) < 100:  # ê¸¸ì´ ì œí•œ
                    news_titles.append(news_title)
        
        # ìƒìœ„ 5ê°œë§Œ ì„ íƒ (ë¶€í•˜ ê°ì†Œ)
        news_titles = news_titles[:5]
        
        if news_titles:
            result = ', '.join(news_titles)
            print(f"âœ… êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_titles)}ê°œ ë‰´ìŠ¤ ì œëª©")
            return result
        else:
            print("âš ï¸ êµ¬ê¸€ íŠ¸ë Œë“œì—ì„œ ë‰´ìŠ¤ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""
            
    except requests.RequestException as e:
        print(f"âŒ êµ¬ê¸€ íŠ¸ë Œë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
        return ""
    except ET.ParseError as e:
        print(f"âŒ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        return ""
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        # ì„¸ì…˜ ì •ë¦¬
        if 'session' in locals():
            session.close()


def search_web_content(search_keywords, max_results=3):
    """
    ì›¹ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ë‚´ìš©ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        search_keywords (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        list: ìˆ˜ì§‘ëœ ì›¹ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸ (dict í˜•íƒœ)
    """
    try:
        print(f"ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘: {search_keywords}")
        
        # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
        search_results = []
        
        # 1. Bing ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
        bing_results = search_bing(search_keywords, max_results)
        search_results.extend(bing_results)
        
        # 2. Naver ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
        naver_results = search_naver(search_keywords, max_results)
        search_results.extend(naver_results)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_results = remove_duplicate_results(search_results)
        
        # ìµœëŒ€ ê²°ê³¼ ìˆ˜ë¡œ ì œí•œ
        final_results = unique_results[:max_results]
        
        print(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
        return final_results
        
    except Exception as e:
        print(f"âŒ ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def search_bing(search_keywords, max_results=3):
    """Bing ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘"""
    try:
        # Bing ê²€ìƒ‰ URL ìƒì„±
        encoded_keywords = quote_plus(search_keywords)
        search_url = f"https://www.bing.com/search?q={encoded_keywords}&format=rss"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(search_url, timeout=10)
        response.raise_for_status()
        
        # RSS íŒŒì‹±
        root = ET.fromstring(response.content)
        results = []
        
        items = root.findall('.//item')[:max_results]
        for item in items:
            title_elem = item.find('title')
            link_elem = item.find('link')
            description_elem = item.find('description')
            
            if title_elem is not None and link_elem is not None:
                title = title_elem.text.strip() if title_elem.text else ""
                url = link_elem.text.strip() if link_elem.text else ""
                description = description_elem.text.strip() if description_elem and description_elem.text else ""
                
                # ì›¹ í˜ì´ì§€ ë‚´ìš© ìˆ˜ì§‘
                content = extract_web_content(url)
                
                results.append({
                    'title': title,
                    'url': url,
                    'description': description,
                    'content': content,
                    'source': 'bing'
                })
        
        session.close()
        return results
        
    except Exception as e:
        print(f"âŒ Bing ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def search_naver(search_keywords, max_results=3):
    """Naver ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘"""
    try:
        # Naver ê²€ìƒ‰ URL ìƒì„±
        encoded_keywords = quote_plus(search_keywords)
        search_url = f"https://search.naver.com/search.naver?where=news&query={encoded_keywords}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(search_url, timeout=10)
        response.raise_for_status()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # ë‰´ìŠ¤ ê²°ê³¼ ì¶”ì¶œ
        news_items = soup.find_all('div', class_='news_wrap')[:max_results]
        
        for item in news_items:
            title_elem = item.find('a', class_='news_tit')
            link_elem = item.find('a', class_='news_tit')
            description_elem = item.find('div', class_='news_dsc')
            
            if title_elem and link_elem:
                title = title_elem.get_text(strip=True)
                url = link_elem.get('href', '')
                description = description_elem.get_text(strip=True) if description_elem else ""
                
                # ì›¹ í˜ì´ì§€ ë‚´ìš© ìˆ˜ì§‘
                content = extract_web_content(url)
                
                results.append({
                    'title': title,
                    'url': url,
                    'description': description,
                    'content': content,
                    'source': 'naver'
                })
        
        session.close()
        return results
        
    except Exception as e:
        print(f"âŒ Naver ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def extract_web_content(url, max_length=1000):
    """
    ì›¹ í˜ì´ì§€ì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        url (str): ì›¹ í˜ì´ì§€ URL
        max_length (int): ìµœëŒ€ ì¶”ì¶œ ê¸¸ì´
    
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë‚´ìš©
    """
    try:
        if not url or not url.startswith('http'):
            return ""
        
        # Playwright ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if PLAYWRIGHT_AVAILABLE:
            return extract_content_with_playwright(url, max_length)
        else:
            return extract_content_with_requests(url, max_length)
            
    except Exception as e:
        print(f"âŒ ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def extract_content_with_playwright(url, max_length=1000):
    """Playwrightë¥¼ ì‚¬ìš©í•œ ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # í˜ì´ì§€ ë¡œë“œ
            page.goto(url, timeout=15000)
            page.wait_for_timeout(2000)
            
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš° iframe ì²˜ë¦¬
            if "blog.naver.com" in url:
                try:
                    page.wait_for_selector("iframe#mainFrame", timeout=5000)
                    frame = page.frame(name="mainFrame")
                    if frame:
                        html = frame.content()
                    else:
                        html = page.content()
                except:
                    html = page.content()
            else:
                html = page.content()
            
            browser.close()
            
            # HTML íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
            content_selectors = [
                'article', 'main', '.content', '.post-content', '.entry-content',
                '.article-content', '.post-body', '.entry-body', '.main-content'
            ]
            
            content_text = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    for element in elements:
                        text = element.get_text(separator=' ', strip=True)
                        if len(text) > len(content_text):
                            content_text = text
            
            # ì£¼ìš” ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            if not content_text:
                content_text = soup.get_text(separator=' ', strip=True)
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content_text = re.sub(r'\s+', ' ', content_text)
            content_text = content_text.strip()
            
            # ê¸¸ì´ ì œí•œ
            if len(content_text) > max_length:
                content_text = content_text[:max_length] + "..."
            
            return content_text
            
    except Exception as e:
        print(f"âŒ Playwright ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def extract_content_with_requests(url, max_length=1000):
    """Requestsë¥¼ ì‚¬ìš©í•œ ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(url, timeout=10)
        response.raise_for_status()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator=' ', strip=True)
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ê¸¸ì´ ì œí•œ
        if len(text) > max_length:
            text = text[:max_length] + "..."
        
        session.close()
        return text
        
    except Exception as e:
        print(f"âŒ Requests ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def remove_duplicate_results(results):
    """ì¤‘ë³µëœ ê²€ìƒ‰ ê²°ê³¼ ì œê±°"""
    seen_urls = set()
    unique_results = []
    
    for result in results:
        url = result.get('url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_results.append(result)
    
    return unique_results


def convert_video_to_mp4_and_upload(video_path, max_duration=8, fps=10, width=800):
    """
    ë¹„ë””ì˜¤ë¥¼ MP4 â†’ GIFë¡œ ë³€í™˜í•˜ê³  GitHubì— ì—…ë¡œë“œ
    
    Args:
        video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        max_duration (int): ìµœëŒ€ ê¸¸ì´ (ì´ˆ)
        fps (int): í”„ë ˆì„ ìˆ˜
        width (int): ë„ˆë¹„ (ë†’ì´ëŠ” ë¹„ìœ¨ì— ë§ì¶° ìë™ ì¡°ì •)
    
    Returns:
        tuple: (github_url, thumb_url) ë˜ëŠ” (None, None) - í•­ìƒ GIF URL ë°˜í™˜
    """
    if not MOVIEPY_AVAILABLE:
        print("âŒ moviepy ëª¨ë“ˆì´ ì—†ì–´ ë¹„ë””ì˜¤ ë³€í™˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    try:
        import tempfile
        
        print(f"ğŸ¬ MP4 â†’ GIF ë³€í™˜ ì‹œì‘: {video_path}")
        print(f"   ì„¤ì •: ìµœëŒ€ {max_duration}ì´ˆ, {fps}fps, ë„ˆë¹„ {width}px")
        
        # ğŸ†• íŒŒì¼ ìë™ ìˆ˜ì • ì‹œë„
        try:
            from video_converter import auto_fix_video_file
            fixed_video_path = auto_fix_video_file(video_path)
            
            if fixed_video_path and fixed_video_path != video_path:
                print(f"ğŸ”§ íŒŒì¼ ìˆ˜ì •ë¨: {video_path} â†’ {fixed_video_path}")
                video_path = fixed_video_path
        except ImportError:
            print("âš ï¸ video_converter ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¹„ë””ì˜¤ ë¡œë“œ
        with VideoFileClip(video_path) as video:
            # ê¸¸ì´ ì œí•œ
            if video.duration > max_duration:
                video = video.subclip(0, max_duration)
            
            # í•´ìƒë„ ì¡°ì •
            if video.w > width:
                video = video.resize(width=width)
            
            # ì„ì‹œ MP4 íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mp4_filename = f"video_{timestamp}.mp4"
            mp4_path = os.path.join("E:/Ai project/nb_wfa/chatbot/image", mp4_filename)
            
            # MP4ë¡œ ì €ì¥
            video.write_videofile(
                mp4_path,
                fps=fps,
                codec='libx264',
                audio_codec='aac',
                temp_audiofile='temp-audio.m4a',
                remove_temp=True
            )
            
            print(f"âœ… MP4 ë³€í™˜ ì™„ë£Œ: {mp4_path}")
            
            # ğŸ†• MP4 â†’ GIF ë³€í™˜
            gif_filename = f"video_{timestamp}.gif"
            gif_path = os.path.join("E:/Ai project/nb_wfa/chatbot/image", gif_filename)
            
            print(f"ğŸï¸ GIF ë³€í™˜ ì‹œì‘: {mp4_path} â†’ {gif_path}")
            
            # GIFë¡œ ë³€í™˜ (ë” ë‚®ì€ fpsë¡œ íŒŒì¼ í¬ê¸° ì¤„ì´ê¸°)
            gif_fps = min(fps, 8)  # GIFëŠ” 8fps ì´í•˜ ê¶Œì¥
            video.write_gif(
                gif_path,
                fps=gif_fps,
                program='ffmpeg',
                opt='optimizeplus'
            )
            
            print(f"âœ… GIF ë³€í™˜ ì™„ë£Œ: {gif_path}")
            
            # ğŸ—‘ï¸ ì„ì‹œ MP4 íŒŒì¼ ì‚­ì œ
            try:
                os.remove(mp4_path)
                print(f"ğŸ—‘ï¸ ì„ì‹œ MP4 íŒŒì¼ ì‚­ì œ: {mp4_path}")
            except:
                pass
            
            # GIFë¥¼ GitHubì— ì—…ë¡œë“œ
            try:
                from github_uploader import upload_image_to_github
                result = upload_image_to_github(gif_path, gif_filename, save_thumb=False)
                if isinstance(result, tuple):
                    github_url, thumb_url = result
                else:
                    github_url = result
                    thumb_url = None
                
                print(f"âœ… GIF GitHub ì—…ë¡œë“œ ì™„ë£Œ: {github_url}")
                return github_url, thumb_url
            except ImportError:
                print("âš ï¸ github_uploader ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
            
    except Exception as e:
        print(f"âŒ MP4 â†’ GIF ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ğŸ”§ ìë™ ìˆ˜ì • ì‹œë„ ì¤‘...")
        
        # ğŸ†• ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ìˆ˜ì • ì¬ì‹œë„
        try:
            from video_converter import auto_fix_video_file
            fixed_video_path = auto_fix_video_file(video_path)
            
            if fixed_video_path and fixed_video_path != video_path:
                print(f"ğŸ”„ ìˆ˜ì •ëœ íŒŒì¼ë¡œ ì¬ì‹œë„: {fixed_video_path}")
                return convert_video_to_mp4_and_upload(fixed_video_path, max_duration, fps, width)
        except Exception as fix_error:
            print(f"âŒ ìë™ ìˆ˜ì •ë„ ì‹¤íŒ¨: {fix_error}")
        
        return None, None


def get_driver(headless=True):
    """Selenium ë“œë¼ì´ë²„ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    except ImportError:
        print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None


def draw_caption_with_shadow(img, text, font_path=r"E:\Ai project\nb_wfa\chatbot\full_screenshot\NanumGothic.ttf", font_size=20, padding=50):
    """ì´ë¯¸ì§€ì— ìë§‰ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    from PIL import ImageDraw, ImageFont
    
    # í•˜ë‹¨ì— ê²€ì€ìƒ‰ ë°°ê²½ ì¶”ê°€ (ì´ë¯¸ì§€ ë†’ì´ ì¦ê°€)
    new_height = img.height + padding
    new_img = Image.new("RGB", (img.width, new_height), color="black")
    new_img.paste(img, (0, 0))

    draw = ImageDraw.Draw(new_img)

    # í°íŠ¸ ì„¤ì •
    try:
        font = ImageFont.truetype(font_path, font_size)
    except:
        font = ImageFont.load_default()
        print("âš ï¸ NanumGothic.ttf í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
    try:
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
    except AttributeError:
        text_width, text_height = font.getsize(text)

    # í…ìŠ¤íŠ¸ ìœ„ì¹˜: í•˜ë‹¨ ì¤‘ì•™
    x = (new_img.width - text_width) // 2
    y = img.height + (padding - text_height) // 2

    # ê·¸ë¦¼ì íš¨ê³¼
    for dx in [-1, 1]:
        for dy in [-1, 1]:
            draw.text((x + dx, y + dy), text, font=font, fill="gray")

    # ì‹¤ì œ í…ìŠ¤íŠ¸
    draw.text((x, y), text, font=font, fill="white")

    return new_img


def is_image_similar(img1, img2, threshold=0.8):
    """ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜ ìœ ì‚¬ë„ ë¹„êµ"""
    import imagehash
    hash1 = imagehash.average_hash(img1)
    hash2 = imagehash.average_hash(img2)
    similarity = 1 - (hash1 - hash2) / len(hash1.hash) ** 2
    return similarity >= threshold


def count_text_characters_in_image(image):
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë¬¸ì ìˆ˜ë¥¼ ì„¸ëŠ” í•¨ìˆ˜"""
    # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” OCRì´ í•„ìš”
    return 0, ""


def download_bing_images_for_sora(search_query, max_images=5, output_filename="bing_sora_reference.png"):
    """
    Bing ì´ë¯¸ì§€ë¥¼ Sora ChatGPTì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
        search_query (str): ê²€ìƒ‰ì–´
        max_images (int): ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ (Soraìš©ìœ¼ë¡œëŠ” ì ì€ ìˆ˜ê°€ ì í•©)
        output_filename (str): ì¶œë ¥ íŒŒì¼ëª…
    
    Returns:
        dict: Sora ChatGPTì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì •ë³´
    """
    print(f"ğŸ¬ Bing + Sora ëª¨ë“œ ì‹œì‘: {search_query}")

    import random, urllib.parse, json, time
    import requests
    from io import BytesIO
    from PIL import Image

    used_image_urls = set()
    used_images = []

    driver = get_driver(headless=False)
    if driver is None:
        return None
        
    images_downloaded = []

    try:
        from selenium.webdriver.common.by import By
        
        search_query_with_date = f"{search_query}"
        encoded_query = urllib.parse.quote_plus(search_query_with_date)
        bing_url = f"https://www.bing.com/images/search?q={encoded_query}&form=HDRSC3"
        print("ğŸ” Bing ì´ë¯¸ì§€ ê²€ìƒ‰ URL:", bing_url)
        driver.get(bing_url)
        time.sleep(3)

        print("ğŸ” ì¼ë°˜ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
        general_images = []
        thumb_items = driver.find_elements(By.CSS_SELECTOR, "a.iusc")
        for a in thumb_items:
            try:
                metadata = a.get_attribute("m")
                if metadata:
                    meta_json = json.loads(metadata)
                    image_url = meta_json.get("murl")
                    title = meta_json.get("t", "")
                    if image_url and image_url.startswith("http") and image_url not in used_image_urls:
                        general_images.append({"src": image_url, "title": title, "source": "bing"})
                if len(general_images) >= max_images:
                    break
            except:
                continue
        images_downloaded.extend(general_images)

    finally:
        driver.quit()

    if not images_downloaded:
        print("âŒ ë‹¤ìš´ë¡œë“œ ì„±ê³µí•œ ì´ë¯¸ì§€ ì—†ìŒ")
        return None

    grid_num = min(3, len(images_downloaded))  # Soraìš©ìœ¼ë¡œëŠ” 3ê°œ ì •ë„ê°€ ì í•©
    print(f"ğŸ¯ Soraìš©ìœ¼ë¡œ {grid_num}ê°œì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.")

    valid_imgs = []
    valid_info = []

    for item in images_downloaded:
        src = item["src"]
        title = item["title"]
        source = item.get("source", "unknown")

        if src in used_image_urls:
            print(f"âš ï¸ ì¤‘ë³µ ì´ë¯¸ì§€ URL ê±´ë„ˆëœ€: {src}")
            continue

        try:
            resp = requests.get(src, timeout=10)
            img = Image.open(BytesIO(resp.content)).convert("RGB")
        except Exception:
            continue

        if any(is_image_similar(img, used_img, threshold=0.8) for used_img in used_images):
            print("âš ï¸ ì´ë¯¸ì§€ ìì²´ ìœ ì‚¬ë„ 80% ì´ìƒ â†’ ì¤‘ë³µ ì²˜ë¦¬ë¨")
            continue

        print("ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
        count, raw_text = count_text_characters_in_image(img)

        print("âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ")
        caption = ''
        print("ğŸ“ ì´ë¯¸ì§€ ì œëª©:", title)
        print("ğŸ“ ì´ë¯¸ì§€ ì¶œì²˜:", source)
        print(f"ğŸ“ ì´ë¯¸ì§€ URL: {src}")

        # Sora ëª¨ë“œ: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘ (ì£¼ì œ ì¼ì¹˜ ê²€ì‚¬ ì—†ìŒ)
        valid_imgs.append(img)
        valid_info.append({
            "img": img,
            "url": src,
            "title": title,
            "caption": caption,
            "match_result": {"result": "sora_mode", "reason": "Sora ëª¨ë“œë¡œ ìë™ ìˆ˜ì§‘"},
            "source": source
        })
        used_image_urls.add(src)
        used_images.append(img)
        print(f"âœ… [{source}] Sora ëª¨ë“œë¡œ ì´ë¯¸ì§€ ì¶”ê°€ë¨: {title}")

        if len(valid_imgs) >= grid_num:
            print("âœ… Soraìš© ì´ë¯¸ì§€ ìˆ˜ëŸ‰ ì¶©ì¡± â†’ ì¤‘ë‹¨")
            break

    if not valid_info:
        print("âŒ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ì´ë¯¸ì§€ ê°œìˆ˜ì— ë”°ë¥¸ ë ˆì´ì•„ì›ƒ ê²°ì •
    image_count = len(valid_info)
    
    if image_count == 1:
        # ì´ë¯¸ì§€ê°€ 1ì¥ì¸ ê²½ìš°: ì›ë³¸ í¬ê¸° ìœ ì§€
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ 1ì¥: ì›ë³¸ í¬ê¸° ìœ ì§€")
        img = valid_info[0]["img"]
        grid_img = img.copy()
    else:
        # ì´ë¯¸ì§€ê°€ 2ì¥ ì´ìƒì¸ ê²½ìš°: ê°€ë¡œë¡œ ë¶™ì´ê¸°
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ {image_count}ì¥: ê°€ë¡œë¡œ ë¶™ì´ê¸°")
        
        # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ ë†’ì´ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        target_height = min(im["img"].height for im in valid_info)
        cell_imgs = []
        
        for im_info in valid_info:
            img = im_info["img"]
            # ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ë†’ì´ì— ë§ì¶° ë¦¬ì‚¬ì´ì¦ˆ
            aspect_ratio = img.width / img.height
            new_width = int(target_height * aspect_ratio)
            resized_img = img.resize((new_width, target_height), Image.LANCZOS)
            cell_imgs.append(resized_img)
        
        # ê°€ë¡œë¡œ ì´ë¯¸ì§€ë“¤ì„ ë¶™ì´ê¸°
        total_width = sum(img.width for img in cell_imgs)
        grid_img = Image.new("RGB", (total_width, target_height), (255, 255, 255))
        
        x_offset = 0
        for img in cell_imgs:
            grid_img.paste(img, (x_offset, 0))
            x_offset += img.width
        
        # ëª©í‘œ ë„ˆë¹„ì— ë§ì¶° ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
        target_width = 1024
        if total_width > target_width:
            aspect_ratio = total_width / target_height
            new_height = int(target_width / aspect_ratio)
            grid_img = grid_img.resize((target_width, new_height), Image.LANCZOS)
        else:
            # ì‘ì€ ê²½ìš° ëª©í‘œ ë„ˆë¹„ì— ë§ì¶° í™•ëŒ€
            aspect_ratio = total_width / target_height
            new_height = int(target_width / aspect_ratio)
            grid_img = grid_img.resize((target_width, new_height), Image.LANCZOS)

    # Soraìš© ìë§‰ ìƒì„±
    combined_caption = " / ".join(
        f"Bing ì´ë¯¸ì§€ {i+1}: {info['title'][:30]}..." 
        for i, info in enumerate(valid_info)
    )
    subtitle = f"Sora ëª¨ë“œ: {search_query[:50]}..."
    
    grid_img = draw_caption_with_shadow(grid_img, subtitle)
    grid_img.save(output_filename)
    
    # ì´ë¯¸ì§€ ê°œìˆ˜ì— ë”°ë¥¸ ì¶œë ¥ ë©”ì‹œì§€
    if image_count == 1:
        print(f"[ë‹¨ì¼ ì´ë¯¸ì§€ ì €ì¥] {output_filename}")
    else:
        print(f"[ê°€ë¡œ ë°°ì¹˜ {image_count}ì¥ ì´ë¯¸ì§€ ì €ì¥] {output_filename}")

    # Sora ChatGPTìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    sora_prompts = [
        f"ì´ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ {search_query}ì— ëŒ€í•œ ì˜ìƒì„ ìƒì„±í•´ì£¼ì„¸ìš”. ì´ë¯¸ì§€ì˜ ë¶„ìœ„ê¸°ì™€ êµ¬ë„ë¥¼ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
        for _ in valid_info
    ]

    result_data = {
        "grid_path": output_filename,
        "mode": "sora",
        "images": [
            {
                "url": info["url"],
                "title": info["title"],
                "caption": info["caption"],
                "match_result": info["match_result"],
                "source": info["source"]
            }
            for info in valid_info
        ],
        "used_image_urls": list(used_image_urls),
        "sora_prompts": sora_prompts
    }

    print("ğŸ¬ Sora ChatGPTìš© í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
    print("âœ… Sora ëª¨ë“œ ì™„ë£Œ!")
    print("ğŸ“‹ Sora ChatGPTì—ì„œ ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸:")
    for i, prompt in enumerate(sora_prompts, 1):
        print(f"  {i}. {prompt}")
    
    print(f"ğŸ–¼ï¸ ì°¸ê³  ì´ë¯¸ì§€ ì €ì¥ë¨: {output_filename}")
    print("ğŸ’¡ Sora ChatGPTì—ì„œ ì´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ìœ„ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!")

    return result_data


def collect_trending_articles_as_text():
    """
    íŠ¸ë Œë”© ê¸°ì‚¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    í˜„ì¬ëŠ” collect_google_trends í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ êµ¬ê¸€ íŠ¸ë Œë“œë¥¼ ë°˜í™˜
    """
    return collect_google_trends()


def generate_search_link(keyword, search_engine="bing"):
    """
    ê²€ìƒ‰ ì—”ì§„ì— ë”°ë¥¸ ê²€ìƒ‰ ë§í¬ ìƒì„±
    
    Args:
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_engine (str): ê²€ìƒ‰ ì—”ì§„ ("bing", "naver", "google")
    
    Returns:
        str: ê²€ìƒ‰ URL
    """
    from urllib.parse import quote
    
    encoded_keyword = quote(keyword)
    
    if search_engine.lower() == "naver":
        return f"https://search.naver.com/search.naver?query={encoded_keyword}"
    elif search_engine.lower() == "google":
        return f"https://www.google.com/search?q={encoded_keyword}"
    else:  # bing (ê¸°ë³¸ê°’)
        return f"https://www.bing.com/search?q={encoded_keyword}&sendquery=1&FORM=SCCODX&rh=B0D80A4F&ref=rafsrchae" 