"""
ë¸”ë¡œê·¸ ìƒì„±ê¸° - gpt_chat_interface.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹
gpt_chat_interface.pyì˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
"""

import sys
import os
import json
import re
import json5
from urllib.parse import quote
from openai import OpenAI

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ)
import os
api_key = os.getenv("OPENAI_API_KEY", "")
if not api_key:
    # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
    try:
        import json
        config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "openai_config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = config.get("api_key", "")
    except:
        pass
client = OpenAI(api_key=api_key) if api_key else None


class BlogGeneratorGPTStyle:
    """gpt_chat_interface.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë¸”ë¡œê·¸ ìƒì„± ë°©ì‹"""
    
    def __init__(self, config=None):
        """ì´ˆê¸°í™”"""
        self.config = config or {
            "chat_model": "gpt-4o-mini",
            "image_source": "bing"
        }
        self.collected_web_data = ""
        self.collected_urls = []
        self._current_coupang_product = None
    
    def call_chat_with_fallback(self, messages, primary_model="gpt-4o-mini", temperature=0.3, max_tokens=500):
        """ëª¨ë¸/íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± ì²˜ë¦¬"""
        model_candidates = [primary_model]
        token_param_candidates = [None, "max_tokens", "max_completion_tokens"]
        temperature_modes = [
            ("given", temperature),
            ("one", 1),
            ("omit", None),
        ]
        for model_name in model_candidates:
            for token_param in token_param_candidates:
                for temp_mode, temp_value in temperature_modes:
                    try:
                        params = {
                            "model": model_name,
                            "messages": messages,
                        }
                        if temp_mode != "omit":
                            params["temperature"] = temp_value
                        if token_param is not None and max_tokens is not None:
                            params[token_param] = max_tokens
                        return client.chat.completions.create(**params)
                    except Exception as e:
                        err = str(e)
                        if (
                            "Unsupported parameter" in err
                            and ("max_tokens" in err or "max_completion_tokens" in err)
                        ):
                            print(f"âš ï¸ ëª¨ë¸ '{model_name}'ì—ì„œ '{token_param}' ë¯¸ì§€ì› â†’ ëŒ€ì²´ í† í° íŒŒë¼ë¯¸í„° ì‹œë„")
                            break
                        if (
                            "Unsupported value" in err and "temperature" in err
                        ):
                            print(f"âš ï¸ ëª¨ë¸ '{model_name}'ì—ì„œ temperature ê°’ ë¯¸ì§€ì› â†’ ëŒ€ì²´ temperature ëª¨ë“œ ì‹œë„")
                            continue
                        if (
                            "model_not_found" in err
                            or "does not have access" in err
                            or "403" in err
                        ):
                            print(f"âš ï¸ ëª¨ë¸ '{model_name}' ì‚¬ìš© ë¶ˆê°€, ë‹¤ìŒ í›„ë³´ë¡œ í´ë°±: {err}")
                            break
                        raise
        raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í—ˆìš© ëª¨ë¸ ë° ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
    
    def gpt(self, user_content: str, system_content: str = None, temperature: float = 0.3,
            max_tokens: int = 500, primary_model: str = None) -> str:
        """ë‹¨ì¼ GPT í˜¸ì¶œ í•¨ìˆ˜: system/userë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ ì‘ë‹µ(content)ë§Œ ë°˜í™˜"""
        messages = []
        if system_content:
            messages.append({"role": "system", "content": system_content})
        messages.append({"role": "user", "content": user_content})
        if not primary_model:
            primary_model = self.config.get("chat_model", "gpt-4o-mini")
        resp = self.call_chat_with_fallback(
            messages=messages,
            primary_model=primary_model,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        return (resp.choices[0].message.content or "").strip()
    
    def extract_json_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ë¸”ë¡ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        try:
            print(f"ğŸ” JSON ì¶”ì¶œ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
            
            # 1. ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
            code_block_patterns = [
                r"```json\s*(\{[\s\S]*?\})\s*```",
                r"```\s*(\{[\s\S]*?\})\s*```",
                r"`(\{[\s\S]*?\})`"
            ]
            
            for pattern in code_block_patterns:
                match = re.search(pattern, text)
                if match:
                    json_str = match.group(1)
                    try:
                        json.loads(json_str)
                        print(f"âœ… ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ")
                        return json_str
                    except:
                        continue
            
            # 2. ì¤‘ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ JSON ê°ì²´ ì¶”ì¶œ
            json_patterns = [
                r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}",
                r"\{[^}]*\}",
                r"\{[\s\S]*?\}"
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    try:
                        json.loads(match)
                        print(f"âœ… ì •ê·œì‹ íŒ¨í„´ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ")
                        return match
                    except:
                        continue
            
            # 3. í…ìŠ¤íŠ¸ ì •ë¦¬ í›„ ì¬ì‹œë„
            cleaned_text = text
            cleaned_text = re.sub(r'[^\x00-\x7F]+', '', cleaned_text)
            cleaned_text = re.sub(r"^\s*[*\-+]\s*", "", cleaned_text, flags=re.MULTILINE)
            cleaned_text = re.sub(r"^\s*#+\s*", "", cleaned_text, flags=re.MULTILINE)
            cleaned_text = re.sub(r'\n\s*\n', '\n', cleaned_text)
            
            for pattern in json_patterns:
                matches = re.findall(pattern, cleaned_text)
                for match in matches:
                    try:
                        json.loads(match)
                        print(f"âœ… ì •ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ")
                        return match
                    except:
                        continue
            
            # 4. ë§ˆì§€ë§‰ ì‹œë„: í‚¤ì›Œë“œ ê¸°ë°˜
            if '"final_title"' in text or '"section_titles"' in text:
                start_idx = max(0, text.find('{'))
                end_idx = text.rfind('}') + 1
                if start_idx < end_idx:
                    potential_json = text[start_idx:end_idx]
                    try:
                        json.loads(potential_json)
                        print(f"âœ… í‚¤ì›Œë“œ ê¸°ë°˜ JSON ì¶”ì¶œ ì„±ê³µ")
                        return potential_json
                    except:
                        pass
            
            print(f"âŒ JSON ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ë°©ë²• ì‹œë„ ì™„ë£Œ")
            return None
            
        except Exception as e:
            print(f"JSON ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_section_titles_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ ì œëª©ì„ ì¶”ì¶œí•˜ëŠ” ëŒ€ì²´ ë°©ë²•"""
        try:
            print(f"ğŸ” í…ìŠ¤íŠ¸ì—ì„œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì‹œë„...")
            
            patterns = [
                r'(\d+\.\s*[^\n]+)',
                r'(\d+\)\s*[^\n]+)',
                r'([A-Z][^.\n]+\.)',
                r'([ê°€-í£][^.\n]+ì—\s+ëŒ€í•´)',
                r'([ê°€-í£][^.\n]+ì˜\s+íŠ¹ì§•)',
                r'([ê°€-í£][^.\n]+ë°©ë²•)',
            ]
            
            titles = []
            for pattern in patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    title = match.strip()
                    if len(title) > 3 and len(title) < 50:
                        titles.append(title)
            
            unique_titles = []
            seen_titles = set()
            for title in titles:
                normalized = title.strip().lower()
                if normalized and normalized not in seen_titles:
                    seen_titles.add(normalized)
                    unique_titles.append(title)
            
            if unique_titles:
                result = unique_titles[:5]
                print(f"âœ… ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì„±ê³µ: {result}")
                return result
            
            print(f"âš ï¸ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨, ê¸°ë³¸ ì„¹ì…˜ ì œëª© ìƒì„±")
            default_titles = [
                "ì£¼ìš” íŠ¹ì§•",
                "í•µì‹¬ ë‚´ìš©", 
                "ì¤‘ìš”í•œ í¬ì¸íŠ¸",
                "ì¶”ê°€ ì •ë³´",
                "ê²°ë¡ "
            ]
            return default_titles
            
        except Exception as e:
            print(f"ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_article_structure(self, response_text, keyword=""):
        """GPT ì‘ë‹µì—ì„œ ê¸€ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
        try:
            print(f"ğŸ” ê¸€ êµ¬ì¡° íŒŒì‹± ì‹œì‘ - ì‘ë‹µ ê¸¸ì´: {len(response_text)}")
            
            json_block = self.extract_json_from_text(response_text)
            if not json_block:
                print(f"âŒ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                print(f"ğŸ“„ ì›ë³¸ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:300]}...")
                
                print(f"ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì‹œë„...")
                fallback_titles = self.extract_section_titles_from_text(response_text)
                if fallback_titles:
                    print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì„±ê³µ: {fallback_titles}")
                    generated_title = f"{keyword} - ìƒì„¸ ë¶„ì„ ë° ê°€ì´ë“œ"
                    return fallback_titles, generated_title
                else:
                    raise ValueError("JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            parsed = None
            
            try:
                parsed = json5.loads(json_block)
                print(f"âœ… json5ë¡œ íŒŒì‹± ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ json5 íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            if not parsed:
                try:
                    parsed = json.loads(json_block)
                    print(f"âœ… í‘œì¤€ jsonìœ¼ë¡œ íŒŒì‹± ì„±ê³µ")
                except Exception as e:
                    print(f"âš ï¸ í‘œì¤€ json íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            if not parsed:
                try:
                    cleaned_json = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_block)
                    cleaned_json = re.sub(r'[^\x20-\x7e]', '', cleaned_json)
                    parsed = json.loads(cleaned_json)
                    print(f"âœ… ì •ë¦¬ëœ jsonìœ¼ë¡œ íŒŒì‹± ì„±ê³µ")
                except Exception as e:
                    print(f"âš ï¸ ì •ë¦¬ëœ json íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            if not parsed:
                try:
                    cleaned_json = re.sub(r'[^\x00-\x7F]+', '', json_block)
                    cleaned_json = re.sub(r'[^\x20-\x7e]', '', cleaned_json)
                    parsed = json.loads(cleaned_json)
                    print(f"âœ… ì´ëª¨ì§€ ì œê±° í›„ json íŒŒì‹± ì„±ê³µ")
                except Exception as e:
                    print(f"âš ï¸ ì´ëª¨ì§€ ì œê±° í›„ json íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            if not parsed:
                print(f"âŒ ëª¨ë“  JSON íŒŒì‹± ë°©ë²• ì‹¤íŒ¨")
                print(f"ğŸ“„ JSON ë¸”ë¡: {json_block}")
                raise ValueError("JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            section_titles_raw = parsed.get("section_titles", [])
            final_title = parsed.get("final_title", "âŒ ì—†ìŒ")
            
            section_titles_temp = []
            for title in section_titles_raw:
                if isinstance(title, str):
                    section_titles_temp.append(title)
                else:
                    print(f"âš ï¸ ì˜ëª»ëœ ì„¹ì…˜ ì œëª© íƒ€ì…: {type(title)}, ê°’: {title}")
                    section_titles_temp.append(str(title))
            
            section_titles = []
            seen_titles = set()
            for title in section_titles_temp:
                normalized = title.strip().lower()
                if normalized and normalized not in seen_titles:
                    seen_titles.add(normalized)
                    section_titles.append(title)
                else:
                    print(f"âš ï¸ ì¤‘ë³µëœ ì„¹ì…˜ ì œëª© ì œê±°: {title}")
            
            if not section_titles:
                print(f"âš ï¸ ì„¹ì…˜ ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‹œë„...")
                fallback_titles = self.extract_section_titles_from_text(response_text)
                if fallback_titles:
                    print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ ì„±ê³µ: {fallback_titles}")
                    if final_title == "âŒ ì—†ìŒ" or final_title == "ìë™ ìƒì„±ëœ ì œëª©":
                        final_title = f"{keyword} - ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸"
                    return fallback_titles, final_title
                else:
                    raise ValueError("ì„¹ì…˜ ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤")
            
            if len(section_titles) < len(section_titles_raw):
                print(f"âš ï¸ ì¤‘ë³µ ì œê±°ë¡œ ì¸í•´ ì„¹ì…˜ ì œëª©ì´ {len(section_titles)}ê°œë¡œ ì¤„ì–´ë“¦ (ì›ë³¸: {len(section_titles_raw)}ê°œ)")
            
            print("âœ… JSON íŒŒì‹± ì„±ê³µ!")
            print("ğŸ“Œ section_titles:", section_titles)
            print("ğŸ“Œ final_title:", final_title)
            
            return section_titles, final_title
            
        except Exception as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"ğŸ“„ ì›ë³¸ ì‘ë‹µ: {response_text[:500]}...")
            raise Exception(f"ê¸€ êµ¬ì¡° íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    def create_fallback_section_data(self, section_title, response_text):
        """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì„¹ì…˜ ë°ì´í„° ìƒì„±"""
        try:
            content = response_text.strip()
            content = re.sub(r'^```(?:json)?\s*', '', content)
            content = re.sub(r'\s*```$', '', content)
            content = re.sub(r'^\s*[*\-+]\s*', '', content, flags=re.MULTILINE)
            content = re.sub(r'^\s*#+\s*', '', content, flags=re.MULTILINE)
            
            if len(content) > 50:
                detailed_image_prompt = f"{section_title} ê´€ë ¨ ìƒì„¸í•œ ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜, ê³ í™”ì§ˆ, ìƒì„¸í•œ ë¬˜ì‚¬"
                return {
                    "section_title": section_title,
                    "content": f"<p>{content}</p>",
                    "image_prompt": detailed_image_prompt
                }
            
            return None
            
        except Exception as e:
            print(f"ëŒ€ì²´ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_optimal_search_keywords_for_main(self, keyword):
        """ë©”ì¸ ê²€ìƒ‰ì–´ ìƒì„±ì„ ìœ„í•œ GPT í•¨ìˆ˜"""
        try:
            print(f"ğŸ¤– GPTë¡œ ë©”ì¸ ê²€ìƒ‰ì–´ ìƒì„± ì¤‘: {keyword}")
            
            prompt = f"""
ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ìµœì ì˜ ì›¹ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ğŸ“ ì£¼ì œ: {keyword}

ğŸ“‹ ê²€ìƒ‰ì–´ ìƒì„± ì¡°ê±´:
- í•´ë‹¹ ì£¼ì œë¥¼ ì´í•´í•˜ê³  ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. bing.com ì—ì„œ ì‚¬ìš©í•  ê²€ìƒ‰ì–´ì…ë‹ˆë‹¤.
ìƒì„±ëœ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš” (ì„¤ëª… ì—†ì´):
"""
            
            response = self.call_chat_with_fallback(
                messages=[
                    {"role": "system", "content": "ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                primary_model=self.config.get("chat_model", "gpt-4o-mini"),
                temperature=0.3,
                max_tokens=50
            )
            
            generated_keywords = response.choices[0].message.content.strip()
            generated_keywords = re.sub(r'[^\w\sê°€-í£]', ' ', generated_keywords)
            generated_keywords = ' '.join(generated_keywords.split())
            
            if len(generated_keywords) > 50:
                generated_keywords = ' '.join(generated_keywords.split()[:3])
            
            if not generated_keywords or len(generated_keywords.strip()) < 2:
                print(f"âš ï¸ GPT ë©”ì¸ ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ì–´ ì‚¬ìš©")
                generated_keywords = keyword.strip()
                generated_keywords = re.sub(r'[^\w\sê°€-í£]', ' ', generated_keywords)
                generated_keywords = ' '.join(generated_keywords.split()[:3])
            
            print(f"âœ… ìƒì„±ëœ ê²€ìƒ‰ì–´: {generated_keywords}")
            return generated_keywords
            
        except Exception as e:
            print(f"âŒ ë©”ì¸ ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            fallback_keywords = keyword.strip()
            fallback_keywords = re.sub(r'[^\w\sê°€-í£]', ' ', fallback_keywords)
            fallback_keywords = ' '.join(fallback_keywords.split()[:3])
            return fallback_keywords
    
    def organize_collected_data_with_gpt(self, keyword, collected_data):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ GPTë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
        try:
            print(f"ğŸ¤– ìˆ˜ì§‘ëœ ë°ì´í„° ì •ë¦¬ ì¤‘: {len(collected_data)}ì")
            
            prompt = f"""
ë‹¤ìŒì€ ì›¹ì—ì„œ ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ğŸ¯ **ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­**: {keyword}

ğŸ“‹ **ì •ë¦¬ ì¡°ê±´**:
- ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œ ì„ ë³„
- ì¤‘ë³µ ë‚´ìš© ì œê±°
- í•µì‹¬ ì‚¬ì‹¤ê³¼ ì •ë³´ ìœ„ì£¼ë¡œ ì •ë¦¬
- 2000ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½
- ë¬¸ë‹¨ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬

ğŸ“„ **ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„°**:
{collected_data[:8000]}

ì •ë¦¬ëœ ë°ì´í„°ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš” (ì„¤ëª… ì—†ì´):
"""
            
            response = self.call_chat_with_fallback(
                messages=[
                    {"role": "system", "content": "ì›¹ ë°ì´í„°ë¥¼ ì‚¬ìš©ì ìš”ì²­ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                primary_model=self.config.get("chat_model", "gpt-4o-mini"),
                temperature=0.3,
                max_tokens=2000
            )
            
            organized_data = response.choices[0].message.content.strip()
            print(f"âœ… ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(organized_data)}ì")
            return organized_data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return collected_data[:2000] if len(collected_data) > 2000 else collected_data
    
    def collect_web_data_for_section(self, section_title, keyword, clean_trimmed_text):
        """ì„¹ì…˜ë³„ ë°ì´í„° ì œê³µ (ì´ë¯¸ ì •ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)"""
        try:
            print(f"ğŸ“ ì„¹ì…˜ ë°ì´í„° ì¤€ë¹„ ì¤‘: {section_title}")
            
            organized_data = getattr(self, 'collected_web_data', '')
            
            if organized_data:
                result = {
                    "search_keywords": keyword,
                    "web_contents": [organized_data[:1500]],
                    "urls": getattr(self, 'collected_urls', [f"https://www.bing.com/search?q={keyword}"]),
                    "titles": [f"{section_title} ê´€ë ¨ ì •ë³´"]
                }
                print(f"âœ… ì„¹ì…˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(organized_data)}ì")
                return result
            else:
                return {
                    "search_keywords": keyword,
                    "web_contents": [f"{section_title}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”."],
                    "urls": getattr(self, 'collected_urls', [f"https://www.bing.com/search?q={keyword}"]),
                    "titles": [f"{section_title} ê²€ìƒ‰ ê²°ê³¼"]
                }
            
        except Exception as e:
            print(f"âŒ ì„¹ì…˜ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {
                "search_keywords": keyword,
                "web_contents": [f"{section_title}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”."],
                "urls": getattr(self, 'collected_urls', [f"https://www.bing.com/search?q={keyword}"]),
                "titles": [f"{section_title} ê²€ìƒ‰ ê²°ê³¼"]
            }
    
    def build_section_prompt_with_web_data(self, section_title, final_title, keyword, clean_trimmed_text, collected_data, previous_content=""):
        """ì›¹ ìˆ˜ì§‘ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì„¹ì…˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        user_request_section = f"""
ğŸ¯ **ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ (ê°€ì¥ ì¤‘ìš”)**:
ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì£¼ì œ: "{keyword}"

ì´ ìš”ì²­ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì—¬ ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‚´ìš©ê³¼ ë°©í–¥ì„±ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.

ğŸ“ **ê¸°ì¡´ ì‘ì„±ëœ ë‚´ìš© (ì°¸ê³ ìš©)**:
{previous_content}

ìœ„ ê¸°ì¡´ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ê´€ì ê³¼ ì •ë³´ë¡œ ì „ê°œí•´ì£¼ì„¸ìš”.
"""
        
        context_instruction = ""
        if previous_content and previous_content.strip():
            context_instruction = f"""
ì´ì „ ë‚´ìš©: {previous_content[:500]}{'...' if len(previous_content) > 500 else ''}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        web_data_section = ""
        if collected_data["web_contents"]:
            web_data_section = "ì°¸ê³ í•  ì›¹ ì •ë³´:\n"
            for i, content in enumerate(collected_data["web_contents"][:2], 1):
                limited_content = content[:100] + "..." if len(content) > 100 else content
                web_data_section += f"{i}. {limited_content}\n"
        
        url_section = ""
        if collected_data.get("urls"):
            url_section = "í•µì‹¬ ë‹¨ì–´ ë§í¬ (ë³¸ë¬¸ì—ì„œ ìë™ ì ìš©):\n"
            
            core_terms = []
            title_words = section_title.split()
            for word in title_words:
                if len(word) >= 2:
                    core_terms.append(word)
            
            keyword_words = keyword.split()
            for word in keyword_words:
                if len(word) >= 2 and word not in core_terms:
                    core_terms.append(word)
            
            for i, term in enumerate(core_terms[:5], 1):
                section_keywords = []
                for word in section_title.split():
                    if len(word) >= 2 and word != term:
                        section_keywords.append(word)
                
                keyword_parts = []
                for word in keyword.split():
                    if len(word) >= 2 and word != term:
                        keyword_parts.append(word)
                
                search_components = [term]
                search_components.extend(section_keywords[:2])
                search_components.extend(keyword_parts[:2])
                
                detailed_search = " ".join(search_components)
                
                if len(detailed_search) < 10:
                    detailed_search = f"{term} {keyword} {section_title}"
                
                search_query = quote(detailed_search)
                # ì„¤ì •ì—ì„œ ê²€ìƒ‰ ì—”ì§„ ê°€ì ¸ì˜¤ê¸°
                search_engine = self.config.get("search_engine", "bing").lower()
                try:
                    import sys
                    import os
                    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    if parent_dir not in sys.path:
                        sys.path.insert(0, parent_dir)
                    from utils import generate_search_link
                    search_url = generate_search_link(detailed_search, search_engine)
                except ImportError:
                    # utils ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì§ì ‘ ìƒì„±
                    if search_engine == "naver":
                        search_url = f"https://search.naver.com/search.naver?query={search_query}"
                    elif search_engine == "google":
                        search_url = f"https://www.google.com/search?q={search_query}"
                    else:  # bing (ê¸°ë³¸ê°’)
                        search_url = f"https://www.bing.com/search?q={search_query}&sendquery=1&FORM=SCCODX&rh=B0D80A4F&ref=rafsrchae"
                url_section += f"{i}. {term} â†’ <a href=\"{search_url}\" target=\"_blank\">{term}</a> (ê²€ìƒ‰ì–´: {detailed_search}, ì—”ì§„: {search_engine})\n"

        return f"""
{user_request_section}

'{final_title}' ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ "{section_title}" ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

{context_instruction}

ì£¼ì œ: {keyword}
í‚¤ì›Œë“œ: {clean_trimmed_text}
ê²€ìƒ‰ì–´: {collected_data["search_keywords"]}

{web_data_section}

{url_section}

ì‘ì„± ìš”êµ¬ì‚¬í•­:
- ìµœì†Œ 300ì ì´ìƒ (ê¶Œì¥ 300~500ì)
- ê²½ì–´ì²´ ì‚¬ìš©
- ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´
- ì œëª©ì€ í¬í•¨í•˜ì§€ ë§ê³  ë‚´ìš©ë§Œ ì‘ì„±
- HTML íƒœê·¸ ì‚¬ìš©: <p>ë‚´ìš©</p>, <br>ì¤„ë°”ê¿ˆ
- **í•µì‹¬ ë‹¨ì–´ ë§í¬ê°€ ìˆìœ¼ë©´ ë³¸ë¬¸ì—ì„œ í•´ë‹¹ ë‹¨ì–´ê°€ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ <a href="bing.com?search=í•´ë‹¹ ì£¼ì œë¥¼ ì´í•´í•˜ê³  í•µì‹¬ ì£¼ì œ + ê²€ìƒ‰ì–´ë¥¼ ì¨ì£¼ì„¸ìš”">ë‹¨ì–´</a> í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨**
- **ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ì‘ì„±**

ğŸ“‹ **ììœ ë¡œìš´ ë¬¸ë‹¨ ì‘ì„± ë° ì¤‘ë³µ ë°©ì§€**:
ê¸°ì¡´ì— ì‘ì„±ëœ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ììœ ë¡­ê²Œ ë¬¸ë‹¨ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **ì´ì „ ë‚´ìš© ë¶„ì„**: 
   - ì•ì„œ ì‘ì„±ëœ ëª¨ë“  ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ ì¤‘ë³µë˜ëŠ” ì •ë³´ íŒŒì•…
   - ê¸°ì¡´ì— ì–¸ê¸‰ëœ í•µì‹¬ ì •ë³´ë“¤ì„ ì •ë¦¬

2. **ììœ ë¡œìš´ ì „ê°œ**:
   - ë¬¸ë‹¨ì˜ ì£¼ì œë‚˜ ë°©í–¥ì€ ììœ ë¡­ê²Œ ì„¤ì •
   - ê¸°ì¡´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì •ë³´ ì¶”ê°€
   - ì´ì „ì— ì–¸ê¸‰í•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë¶„ì„, ì˜ˆì¸¡, ê´€ì  ì œê³µ

3. **ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°**:
   - ê¸°ì¡´ ë‚´ìš©ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ ì‘ì„±
   - "ì•ì„œ ì–¸ê¸‰í•œ", "ì´ëŸ¬í•œ ë°°ê²½ì—ì„œ", "ì´ì— ë”í•´" ë“±ì˜ ì—°ê²°ì–´ í™œìš©
   - ê¸°ì¡´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ë˜ ìƒˆë¡œìš´ ë‚´ìš©ìœ¼ë¡œ ì „ê°œ

âš ï¸ **ì¤‘ìš”: ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”!**

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
    "section_title": "{section_title}",
    "content": "HTML í˜•ì‹ì˜ ì„¹ì…˜ ë‚´ìš© (ì œëª© ì œì™¸)",
    "image_prompt": "ì´ ì„¹ì…˜ì„ ìœ„í•œ ìƒì„¸í•œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´, 100ì ì´ìƒ ê¶Œì¥)"
}}
```

**JSON ì‘ë‹µ ê·œì¹™:**
1. ë°˜ë“œì‹œ ```jsonìœ¼ë¡œ ì‹œì‘í•˜ê³  ```ë¡œ ëë‚´ì„¸ìš”
2. JSON ê°ì²´ëŠ” ì •í™•í•œ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”
3. ë¬¸ìì—´ ê°’ì—ëŠ” ë°˜ë“œì‹œ í°ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
4. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
5. JSON í˜•ì‹ë§Œ ì‘ë‹µí•˜ì„¸ìš”

**ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê°€ì´ë“œ:**
- ì„¹ì…˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì‘ì„±
- í•œêµ­ì–´ë¡œ ì‘ì„±
- ìµœì†Œ 100ì ì´ìƒ ê¶Œì¥
- ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìƒì„¸í•œ ì„¤ëª… í¬í•¨
"""
    
    def generate_section_content(self, section_title, final_title, keyword, clean_trimmed_text, i, previous_sections_content=""):
        """ê°œë³„ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì›¹ ìˆ˜ì§‘ + GPT ìƒì„±)"""
        print(f"ğŸ“ [{i+1}] ì„¹ì…˜ '{section_title}' ì›¹ ìˆ˜ì§‘ ë° ë‚´ìš© ìƒì„± ì¤‘...")
        
        try:
            collected_data = self.collect_web_data_for_section(section_title, keyword, clean_trimmed_text)
            section_prompt = self.build_section_prompt_with_web_data(
                section_title, final_title, keyword, clean_trimmed_text, 
                collected_data, previous_sections_content
            )
            
            response_text = self.gpt(
                user_content=section_prompt,
                temperature=0.3,
                max_tokens=700,
            )
            json_block = self.extract_json_from_text(response_text)
            
            if not json_block:
                print(f"âŒ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ì¬ì‹œë„ ì‹œë„")
                retry_prompt = section_prompt + f"\n\nì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ë³¸ë¬¸(content)ì€ ìµœì†Œ 300ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì„¤ëª… ê¸ˆì§€.\n```json\n{{\n  \"section_title\": \"{section_title}\",\n  \"content\": \"HTML í˜•ì‹ì˜ ì„¹ì…˜ ë‚´ìš© (ì œëª© ì œì™¸)\",\n  \"image_prompt\": \"ì´ ì„¹ì…˜ì„ ìœ„í•œ ìƒì„¸í•œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´, 100ì ì´ìƒ ê¶Œì¥)\"\n}}\n```"
                retry_text = self.gpt(
                    user_content=retry_prompt,
                    temperature=0.3,
                    max_tokens=900,
                )
                json_block = self.extract_json_from_text(retry_text)
                if not json_block:
                    print(f"âŒ ì¬ì‹œë„ í›„ì—ë„ JSON ë¸”ë¡ ì—†ìŒ - ëŒ€ì²´ ë°©ë²• ì‹œë„")
                    fallback_data = self.create_fallback_section_data(section_title, retry_text or response_text)
                    if fallback_data:
                        print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì„¹ì…˜ ë°ì´í„° ìƒì„± ì„±ê³µ")
                        return fallback_data
                    else:
                        raise ValueError("JSON í˜•ì‹ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ")

            if json_block and json_block.strip():
                section_data = None
                
                try:
                    section_data = json5.loads(json_block)
                except Exception as e:
                    print(f"âš ï¸ json5 íŒŒì‹± ì‹¤íŒ¨: {e}")
                
                if not section_data:
                    try:
                        section_data = json.loads(json_block)
                    except Exception as e:
                        print(f"âš ï¸ json íŒŒì‹± ì‹¤íŒ¨: {e}")
                
                if not section_data:
                    try:
                        cleaned_json = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_block)
                        cleaned_json = re.sub(r'[^\x20-\x7e]', '', cleaned_json)
                        section_data = json.loads(cleaned_json)
                    except Exception as e:
                        print(f"âš ï¸ ì •ë¦¬ëœ json íŒŒì‹± ì‹¤íŒ¨: {e}")
                
                if not section_data:
                    print(f"âŒ ëª¨ë“  JSON íŒŒì‹± ë°©ë²• ì‹¤íŒ¨")
                    fallback_data = self.create_fallback_section_data(section_title, response_text)
                    if fallback_data:
                        print(f"âœ… ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì„¹ì…˜ ë°ì´í„° ìƒì„± ì„±ê³µ")
                        return fallback_data
                    else:
                        raise ValueError("JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                
                print(f"âœ… ì„¹ì…˜ JSON íŒŒì‹± ì„±ê³µ!")
                
                if not isinstance(section_data, dict):
                    raise ValueError("ì„¹ì…˜ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤")
                
                if "section_title" not in section_data or "content" not in section_data:
                    raise ValueError("ì„¹ì…˜ ë°ì´í„°ì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                
                section_title_parsed = str(section_data.get("section_title", ""))
                content = str(section_data.get("content", ""))
                
                if not section_title_parsed or not content:
                    raise ValueError("ì„¹ì…˜ ì œëª©ì´ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

                try:
                    import re as _re
                    plain = _re.sub(r'<[^>]+>', '', content).strip()
                except Exception:
                    plain = content
                if len(plain) < 300:
                    print(f"âš ï¸ ë³¸ë¬¸ ê¸¸ì´ ë¶€ì¡±({len(plain)}ì) - ì¬ìƒì„± ì‹œë„")
                    reinforce_prompt = section_prompt + "\n\në°˜ë“œì‹œ contentë¥¼ ìµœì†Œ 300ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•˜ê³ , JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."
                    retry_text = self.gpt(
                        user_content=reinforce_prompt,
                        temperature=0.3,
                        max_tokens=900,
                    )
                    retry_json = self.extract_json_from_text(retry_text)
                    if retry_json:
                        try:
                            try:
                                section_data_retry = json5.loads(retry_json)
                            except Exception:
                                section_data_retry = json.loads(retry_json)
                            section_title_parsed = str(section_data_retry.get("section_title", section_title_parsed))
                            content = str(section_data_retry.get("content", content))
                            image_prompt = section_data_retry.get("image_prompt", section_data.get("image_prompt", ""))
                        except Exception:
                            pass
                
                image_prompt = section_data.get("image_prompt", "")
                if image_prompt:
                    print(f"ğŸ¨ ì„¹ì…˜ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸: {image_prompt}")
                else:
                    print(f"âš ï¸ ì„¹ì…˜ì—ì„œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                
                section_data = {
                    "section_title": section_title_parsed,
                    "content": content,
                    "image_prompt": image_prompt
                }
                
                print(f"âœ… ì„¹ì…˜ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
                print(f"   - ì œëª©: {section_title_parsed}")
                print(f"   - ë‚´ìš© ê¸¸ì´: {len(content)}ì")
                print(f"   - ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(image_prompt)}ì")
                
                return section_data
            else:
                raise ValueError("JSON ë¸”ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
        except Exception as e:
            raise Exception(f"ì„¹ì…˜ ë‚´ìš© ìƒì„± ì‹¤íŒ¨: {e}")
    
    def create_section_html_without_image(self, section_data):
        """ì´ë¯¸ì§€ ì—†ì´ ì„¹ì…˜ HTMLì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        section_title = section_data["section_title"]
        content = section_data["content"]
        
        if "<h2>" in content:
            return content
        else:
            html = f"<h2>{section_title}</h2>\n{content}\n"
            return html
    
    def generate_blog_post(self, keyword, product_url=None, coupang_product=None):
        """
        ë©”ì¸ ë¸”ë¡œê·¸ ê¸€ ìƒì„± í•¨ìˆ˜ (gpt_chat_interface.pyì˜ send_to_gptì™€ ë™ì¼)
        
        Args:
            keyword: ë¸”ë¡œê·¸ ì£¼ì œ í‚¤ì›Œë“œ
            product_url: ìƒí’ˆ URL (ì„ íƒ)
            coupang_product: ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì„ íƒ)
        
        Returns:
            tuple: (title, content, category) ë˜ëŠ” None
        """
        try:
            product_keyword = keyword
            self._current_coupang_product = coupang_product
            
            if coupang_product:
                product_name = coupang_product.get("name", coupang_product.get("title", keyword))
                product_keyword = product_name
                print(f"ğŸ›’ ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ì‚¬ìš©: {product_name}")
            
            print(f"ğŸ” ì›¹ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            try:
                import sys
                import os
                
                try:
                    from blog_html_generator.web_search import collect_search_data as web_search_collect
                except ImportError:
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    web_search_path = os.path.join(current_dir, 'blog_html_generator')
                    sys.path.insert(0, web_search_path)
                    from web_search import collect_search_data as web_search_collect
                
                search_keywords = self.generate_optimal_search_keywords_for_main(product_keyword)
                
                print(f"ğŸ” '{search_keywords}' êµ¬ê¸€/ë¹™ ê²€ìƒ‰ ì¤‘...")
                
                collected_data, urls = web_search_collect(
                    search_keywords, 
                    max_results=10, 
                    return_urls=True,
                    product_url=product_url
                )
                self.collected_urls = urls
                print(f"ğŸ”— ìˆ˜ì§‘ëœ URL ëª©ë¡: {urls}")
                
                if not collected_data or len(collected_data) < 100:
                    print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(collected_data) if collected_data else 0}ì")
                    collected_data = collected_data if collected_data else ""
                
                print(f"âœ… ì›¹ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data)}ì")
                
                if collected_data and len(collected_data) >= 100:
                    print("ğŸ¤– ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ GPTë¡œ ì •ë¦¬í•©ë‹ˆë‹¤...")
                    organized_data = self.organize_collected_data_with_gpt(product_keyword, collected_data)
                    self.collected_web_data = organized_data
                else:
                    organized_data = collected_data
                    self.collected_web_data = organized_data
                
                clean_trimmed_text = product_keyword
                
            except Exception as e:
                print(f"âŒ ì›¹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                clean_trimmed_text = product_keyword
            
            print("ğŸ¤– GPTì—ê²Œ ê¸€ ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
            
            try:
                from prompt_templates import get_blog_prompt_template
                prompt = get_blog_prompt_template(product_keyword, clean_trimmed_text)
                
                if coupang_product:
                    product_name = coupang_product.get("name", coupang_product.get("title", ""))
                    product_url_val = coupang_product.get("url", coupang_product.get("link", coupang_product.get("product_url", "")))
                    product_image = coupang_product.get("image", coupang_product.get("image_url", coupang_product.get("thumbnail", "")))
                    product_description = coupang_product.get("description", coupang_product.get("desc", ""))
                    product_price = coupang_product.get("price", coupang_product.get("price_text", ""))
                    
                    coupang_info = f"""
ğŸ›’ **ì¿ íŒ¡ ìƒí’ˆ ì •ë³´**:
- ìƒí’ˆëª…: {product_name}
- ìƒí’ˆ ë§í¬: {product_url_val}
- ìƒí’ˆ ì´ë¯¸ì§€: {product_image}
"""
                    if product_description:
                        coupang_info += f"- ìƒí’ˆ ì„¤ëª…: {product_description}\n"
                    if product_price:
                        coupang_info += f"- ê°€ê²©: {product_price}\n"
                    
                    coupang_info += """
ìœ„ ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ìƒí’ˆì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ìƒí’ˆì˜ íŠ¹ì§•, ì¥ì , ì‚¬ìš©ë²•, ì¶”ì²œ ì´ìœ  ë“±ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ë˜, ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ìƒí’ˆ ë§í¬ì™€ ì´ë¯¸ì§€ëŠ” ê¸€ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.
"""
                    prompt += coupang_info
                    
            except ImportError:
                prompt = f"""
ğŸ¯ **ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ (ê°€ì¥ ì¤‘ìš”)**:
ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì£¼ì œ: "{product_keyword}"

ì´ ìš”ì²­ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì—¬ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‚´ìš©ê³¼ ë°©í–¥ì„±ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.

ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¸”ë¡œê·¸ ì‘ì„±ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì™€ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ 
êµ¬ì¡°í™”ë˜ê³  ì½ê¸° ì‰¬ìš´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ì‘ì„± ìš”êµ¬ì‚¬í•­:**
1. **ì£¼ì œ**: {product_keyword}
2. **í‚¤ì›Œë“œ**: {clean_trimmed_text}
3. **êµ¬ì¡°**: ì œëª©, ì†Œê°œ, ë³¸ë¬¸(4-5ê°œ ì„¹ì…˜), ê²°ë¡ 
4. **ìŠ¤íƒ€ì¼**: ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤
5. **ê¸¸ì´**: ì ì ˆí•œ ë¶„ëŸ‰ (ë„ˆë¬´ ê¸¸ì§€ë„ ì§§ì§€ë„ ì•Šê²Œ)

ğŸ“‹ **ììœ ë¡œìš´ ë¬¸ë‹¨ êµ¬ì„± ë° ì¤‘ë³µ ë°©ì§€**:
ë¬¸ë‹¨ êµ¬ì„±ì€ ììœ ë¡­ê²Œ í•˜ë˜, ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:

1. **ì´ì „ ì‘ì„±ëœ ë‚´ìš© ì°¸ê³ **: ì•ì„œ ì‘ì„±ëœ ëª¨ë“  ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¤‘ë³µì„ ë°©ì§€
2. **ìì—°ìŠ¤ëŸ¬ìš´ ì „ê°œ**: ê¸°ì¡´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì •ë³´ë¥¼ ì¶”ê°€
3. **ì—°ê²°ì„± ìœ ì§€**: "ì•ì„œ ì–¸ê¸‰í•œ", "ì´ëŸ¬í•œ ë°°ê²½ì—ì„œ" ë“±ì˜ ì—°ê²°ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
4. **ìƒˆë¡œìš´ ì •ë³´ ì¤‘ì‹¬**: ì´ì „ì— ì–¸ê¸‰í•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ì •ë³´, ê´€ì , ë¶„ì„ì„ ì œê³µ

ë¬¸ë‹¨ì˜ ì£¼ì œë‚˜ ë°©í–¥ì€ ììœ ë¡­ê²Œ ì„¤ì •í•˜ë˜, ë°˜ë“œì‹œ ì´ì „ ë‚´ìš©ê³¼ì˜ ì¤‘ë³µì„ í”¼í•´ì£¼ì„¸ìš”.

**ìˆ˜ì§‘ëœ ì›¹ ë°ì´í„° í™œìš©:**
ìˆ˜ì§‘ëœ ì›¹ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

ìœ„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ "{product_keyword}" ì£¼ì œì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
                
                if coupang_product:
                    product_name = coupang_product.get("name", coupang_product.get("title", ""))
                    product_url_val = coupang_product.get("url", coupang_product.get("link", coupang_product.get("product_url", "")))
                    product_image = coupang_product.get("image", coupang_product.get("image_url", coupang_product.get("thumbnail", "")))
                    product_description = coupang_product.get("description", coupang_product.get("desc", ""))
                    product_price = coupang_product.get("price", coupang_product.get("price_text", ""))
                    
                    coupang_info = f"""
ğŸ›’ **ì¿ íŒ¡ ìƒí’ˆ ì •ë³´**:
- ìƒí’ˆëª…: {product_name}
- ìƒí’ˆ ë§í¬: {product_url_val}
- ìƒí’ˆ ì´ë¯¸ì§€: {product_image}
"""
                    if product_description:
                        coupang_info += f"- ìƒí’ˆ ì„¤ëª…: {product_description}\n"
                    if product_price:
                        coupang_info += f"- ê°€ê²©: {product_price}\n"
                    
                    coupang_info += """
ìœ„ ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ìƒí’ˆì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ìƒí’ˆì˜ íŠ¹ì§•, ì¥ì , ì‚¬ìš©ë²•, ì¶”ì²œ ì´ìœ  ë“±ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•˜ë˜, ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ìƒí’ˆ ë§í¬ì™€ ì´ë¯¸ì§€ëŠ” ê¸€ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.
"""
                    prompt += coupang_info
            
            prompt += """

ì¤‘ìš”: ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…/í…ìŠ¤íŠ¸ ê¸ˆì§€. ë°˜ë“œì‹œ ì½”ë“œíœìŠ¤ ```json ìœ¼ë¡œ ê°ì‹¸ì„œ ì¶œë ¥.
```json
{
  "section_titles": ["ì„¹ì…˜1 ì œëª©", "ì„¹ì…˜2 ì œëª©", "ì„¹ì…˜3 ì œëª©", "ì„¹ì…˜4 ì œëª©", "ì„¹ì…˜5 ì œëª©"],
  "final_title": "ìµœì¢… ì œëª©"
}
```
"""

            response = self.call_chat_with_fallback(
                messages=[{"role": "user", "content": prompt}],
                primary_model=self.config.get("chat_model", "gpt-4o-mini"),
                temperature=1,
            )
            
            response_text = (response.choices[0].message.content or "").strip()
            if not response_text:
                retry_prompt = "í•„ìˆ˜: ìœ„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… ê¸ˆì§€."
                response = self.call_chat_with_fallback(
                    messages=[
                        {"role": "user", "content": prompt},
                        {"role": "user", "content": retry_prompt}
                    ],
                    primary_model=self.config.get("chat_model", "gpt-4o-mini"),
                    temperature=1,
                )
                response_text = (response.choices[0].message.content or "").strip()
            
            print("âœ… GPT ì‘ë‹µ ë°›ìŒ")
            
            section_titles, final_title = self.parse_article_structure(response_text, product_keyword)
            
            title = final_title
            keywords = [product_keyword]
            
            # GPTë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ
            try:
                category_list = [
                    {"ca_name": "AMERICAAI", "ca_description": "ë¯¸êµ­ ì¤‘ì‹¬ì˜ AI ì •ì±…, ê¸°ìˆ  ë™í–¥ ë° êµ­ê°€ ì „ëµ ë¶„ì„, ë¯¸êµ­ ê´€ë ¨ ëª¨ë“  ì†Œì‹"},
                    {"ca_name": "EUAI", "ca_description": "ìœ ëŸ½ì—°í•© AI ê·œì œ, ìœ¤ë¦¬ì  ê¸°ì¤€ ë° EU AI Act ê´€ë ¨ ì •ë³´"},
                    {"ca_name": "Organizers", "ca_description": "í–‰ì‚¬ ì£¼ìµœì ë° ì¡°ì§ ê´€ë ¨ íŠ¸ë Œë“œ ë° ì¸ë¬¼ ì •ë³´"},
                    {"ca_name": "Courses", "ca_description": "ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ êµìœ¡ ê³¼ì • ë° í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼ ì •ë³´"},
                    {"ca_name": "DramaDetails", "ca_description": "êµ­ë‚´ì™¸ ë“œë¼ë§ˆì˜ ì¤„ê±°ë¦¬, ë°°ìš° ì •ë³´ ë° ì‹œì²­ íŠ¸ë Œë“œ"},
                    {"ca_name": "anime", "ca_description": "ì¼ë³¸ ì• ë‹ˆë©”ì´ì…˜ ì‹ ì‘ ì •ë³´, ë¦¬ë·°, íŒ¬ë¤ ë°˜ì‘, ì›¹íˆ°"},
                    {"ca_name": "GameNews", "ca_description": "êµ­ë‚´ì™¸ ìµœì‹  ê²Œì„ ì†Œì‹ ë° ì—…ë°ì´íŠ¸ ì •ë³´"},
                    {"ca_name": "PokemonBread", "ca_description": "í¬ì¼“ëª¬ë¹µ êµ¿ì¦ˆ, ë ë¶€ë ë¶€ì”° ë° ìˆ˜ì§‘ ì •ë³´"},
                    {"ca_name": "EconomicIndicators", "ca_description": "ì£¼ìš” ê²½ì œ ì§€í‘œ ë° ê¸€ë¡œë²Œ ê¸ˆìœµ ë™í–¥"},
                    {"ca_name": "entertainment", "ca_description": "ì—°ì˜ˆê³„ ì „ë°˜ì˜ ë‰´ìŠ¤, ì´ìŠˆ, ìŠ¤íƒ€ ë™í–¥"},
                    {"ca_name": "entertainmentnews", "ca_description": "ì—°ì˜ˆê³„ ì†ë³´ ì¤‘ì‹¬ì˜ ë‰´ìŠ¤ ì½˜í…ì¸ "},
                    {"ca_name": "movie", "ca_description": "ì‹ ì‘ ì˜í™”, ë°•ìŠ¤ì˜¤í”¼ìŠ¤, ê°ë… ë° ë°°ìš° ì •ë³´"},
                    {"ca_name": "sports", "ca_description": "êµ­ë‚´ì™¸ ìŠ¤í¬ì¸  ê²½ê¸° ê²°ê³¼ ë° ì„ ìˆ˜ ì´ìŠˆ"},
                    {"ca_name": "car", "ca_description": "ìë™ì°¨ ì¶œì‹œ, ì‹œìŠ¹ê¸°, ë¸Œëœë“œ ë¹„êµ ì •ë³´"},
                    {"ca_name": "TourSpots", "ca_description": "êµ­ë‚´ì™¸ ì—¬í–‰ì§€ ì¶”ì²œ, ì²´í—˜ê¸° ë° ê´€ê´‘ ì •ë³´"},
                    {"ca_name": "robot", "ca_description": "ë¡œë´‡ ê¸°ìˆ , ì‚°ì—… ë™í–¥ ë° ìƒí™œ ì† ë¡œë´‡ í™œìš©, ì»´í“¨í„° ë¶€í’ˆ í¬í•¨"},
                    {"ca_name": "politics", "ca_description": "êµ­ë‚´ì™¸ ì •ì¹˜ ë‰´ìŠ¤ ë° ì •ì±… ë¶„ì„"},
                    {"ca_name": "RecommendedVideo", "ca_description": "AI ê¸°ë°˜ ì¶”ì²œ ì˜ìƒ ë° ìœ íŠœë¸Œ í•« ì½˜í…ì¸ "},
                    {"ca_name": "x_file", "ca_description": "ë¯¸ìŠ¤í„°ë¦¬, ìŒëª¨ë¡ , UFO ë“± ê¸°ì´í•œ ì •ë³´ ì½˜í…ì¸ "},
                    {"ca_name": "8bit", "ca_description": "ë³µê³ í’ 8ë¹„íŠ¸ ê²Œì„, ì•„íŠ¸, ìŒì•… ê´€ë ¨ ì½˜í…ì¸ "},
                    {"ca_name": "UserQueryLog", "ca_description": "ì‚¬ìš©ì ì§ˆì˜ ê¸°ë°˜ ì¶”ì²œ í‚¤ì›Œë“œ ë° ë¶„ì„ ê²°ê³¼"},
                    {"ca_name": "CosmeticBrandsInfo", "ca_description": "í™”ì¥í’ˆ ë¸Œëœë“œë³„ íŠ¸ë Œë“œ, ì œí’ˆ ë¦¬ë·° ì •ë³´"},
                    {"ca_name": "FashionMakersList", "ca_description": "êµ­ë‚´ì™¸ íŒ¨ì…˜ ë””ìì´ë„ˆ ë° ë¸Œëœë“œ ì •ë³´"},
                    {"ca_name": "mobilegame", "ca_description": "ëª¨ë°”ì¼ ê²Œì„ ì¶œì‹œ ì •ë³´ ë° ì‚¬ìš©ì ë¦¬ë·°"},
                    {"ca_name": "DongmyoFashionHub", "ca_description": "ë™ë¬˜ íŒ¨ì…˜ íŠ¸ë Œë“œ, ê±°ë¦¬ íŒ¨ì…˜ ë° ì¸ê¸° ìƒí’ˆ ì •ë³´"},
                    {"ca_name": "stock", "ca_description": "ì£¼ì‹ì‹œì¥ ë™í–¥, ì¢…ëª© ë¶„ì„ ë° íˆ¬ì ì „ëµ"},
                    {"ca_name": "googleApp", "ca_description": "Google Play ì•± ì¶”ì²œ, ë¦¬ë·° ë° ìˆœìœ„ ì •ë³´"},
                    {"ca_name": "googleBook", "ca_description": "Google ë„ì„œ í”Œë«í¼ ê¸°ë°˜ ì¶”ì²œ ì±… ë° ë¶„ì„"},
                    {"ca_name": "googleKids", "ca_description": "Google Kidsìš© ì½˜í…ì¸ , êµìœ¡ ì•± ì •ë³´"},
                    {"ca_name": "googleComics", "ca_description": "Google í”Œë«í¼ ê¸°ë°˜ ì›¹íˆ°/ì½”ë¯¹ìŠ¤ ì½˜í…ì¸  ì†Œê°œ"},
                    {"ca_name": "Semiraepaong", "ca_description": "ì„¸ë¯¸ë¼ì—íŒŒì˜¹ ê´€ë ¨ ì´ìŠˆ ë˜ëŠ” íŠ¹ì • ê¸°íš ì½˜í…ì¸ "},
                    {"ca_name": "LiveGameStreams", "ca_description": "ì‹¤ì‹œê°„ ê²Œì„ ìŠ¤íŠ¸ë¦¬ë° ì±„ë„ ë° ì¸ê¸° í´ë¦½"},
                    {"ca_name": "PsychologyResources", "ca_description": "ì‹¬ë¦¬í•™ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤, í…ŒìŠ¤íŠ¸, ì •ì‹  ê±´ê°• ì½˜í…ì¸ "},
                    {"ca_name": "CommGuide", "ca_description": "ì§€ì—­ ì»¤ë®¤ë‹ˆí‹° ì•ˆë‚´, ì´ìš© ê·œì¹™, ìš´ì˜ ê°€ì´ë“œ"},
                    {"ca_name": "usa", "ca_description": "ë¯¸êµ­ ì‚¬íšŒ, ê²½ì œ, ë¬¸í™” ê´€ë ¨ íŠ¸ë Œë“œì™€ ë¶„ì„"},
                    {"ca_name": "afreecatv", "ca_description": "ì•„í”„ë¦¬ì¹´TV ì¸ê¸° ë°©ì†¡, BJ íŠ¸ë Œë“œ ë° ì½˜í…ì¸  ë¶„ì„"}
                ]
                
                category_prompt = f"""
ë‹¤ìŒ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì œëª©ê³¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ğŸ“ ì œëª©: {title}
ğŸ“„ ë‚´ìš© ìš”ì•½: {clean_trimmed_text[:500]}...

ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡:
"""
                
                for cat in category_list:
                    category_prompt += f"- {cat['ca_name']}: {cat['ca_description']}\n"
                
                category_prompt += """
ìœ„ ì¹´í…Œê³ ë¦¬ ì¤‘ì—ì„œ ê°€ì¥ ì í•©í•œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì¹´í…Œê³ ë¦¬ëª…ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: GameNews
"""
                
                recommended_category = self.gpt(
                    user_content=category_prompt,
                    system_content="ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                    temperature=0.3,
                    max_tokens=50,
                )
                
                valid_ca_names = [item["ca_name"] for item in category_list]
                if recommended_category in valid_ca_names:
                    category = recommended_category
                    print(f"ğŸ¤– GPT ì¹´í…Œê³ ë¦¬ ì¶”ì²œ: {category}")
                else:
                    category = "AMERICAAI"
                    print(f"âš ï¸ ì¶”ì²œëœ ì¹´í…Œê³ ë¦¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {recommended_category}, ê¸°ë³¸ê°’ ì‚¬ìš©: {category}")
                    
            except Exception as e:
                category = "AMERICAAI"
                print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©: {category}")
            
            print(f"ğŸ“ ì œëª©: {title}")
            print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {category}")
            print(f"ğŸ·ï¸ í‚¤ì›Œë“œ: {', '.join(keywords)}")

            # 1ë‹¨ê³„: ëª¨ë“  ì„¹ì…˜ ë‚´ìš©ì„ ë¨¼ì € ì™„ì„±
            content_parts = []
            section_data_list = []
            previous_sections_content = ""
            
            for i, section_title in enumerate(section_titles):
                try:
                    section_data = self.generate_section_content(section_title, final_title, keyword, clean_trimmed_text, i, previous_sections_content)
                    section_data_list.append(section_data)
                    
                    html = self.create_section_html_without_image(section_data)
                    content_parts.append(html)
                    
                    if section_data and "content" in section_data:
                        current_section_text = section_data["content"]
                        import re
                        clean_text = re.sub(r'<[^>]+>', '', current_section_text)
                        previous_sections_content += f"\n\n{clean_text}"
                    
                    print(f"âœ… ì„¹ì…˜ {i+1} ë‚´ìš© ìƒì„± ì™„ë£Œ")
                except Exception as e:
                    print(f"âŒ ì„¹ì…˜ {i+1} ë‚´ìš© ìƒì„± ì‹¤íŒ¨: {e}")
                    content_parts.append(f"<h2>{section_titles[i]}</h2>\n<p>ì´ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</p>")
                    section_data_list.append({"section_title": section_titles[i], "content": "ì˜¤ë¥˜ ë°œìƒ"})
            
            content = "\n\n".join(content_parts)
            
            print(f"ğŸ“„ ë‚´ìš© ìƒì„± ì™„ë£Œ: {len(content)}ì")
            
            return title, content, category
            
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    generator = BlogGeneratorGPTStyle()
    result = generator.generate_blog_post("ë„¤ì´ì³ëŸ¬ë¸Œë©”ë ˆ ì¹œí™˜ê²½ ì˜¤ë¦¬ì§€ë„ ìœ ì•„ ì„¸ì œ")
    if result:
        title, content, category = result
        print(f"\nâœ… ìƒì„± ì™„ë£Œ!")
        print(f"ì œëª©: {title}")
        print(f"ì¹´í…Œê³ ë¦¬: {category}")
        print(f"ë‚´ìš© ê¸¸ì´: {len(content)}ì")
    else:
        print("âŒ ìƒì„± ì‹¤íŒ¨")

